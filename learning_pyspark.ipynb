{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gz9Z6GNVnAkHFTQl6amoY0Yu2irx04Nb",
      "authorship_tag": "ABX9TyMoTnppRktFcL1u6YQOGaC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/protmaks/PJ_Colab_Learning_pySpark/blob/main/learning_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Установка pySpark"
      ],
      "metadata": {
        "id": "mHYH6PKi7IBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqt0__pf66ob",
        "outputId": "95c2cb17-649c-46ec-c062-c7ea7dbafde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=3323ec447878fdf58ba7c9161c23a0ed13ae467a6327ca255a8125d7c5d73f83\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Создаём сессию Spark"
      ],
      "metadata": {
        "id": "FWxHssGhfQyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, round, col, count, asc, desc\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "df_path = 'drive/MyDrive/stocks_price_final.csv'\n",
        "df_path2 = 'drive/MyDrive/bank.csv'\n",
        "df_path3 = 'drive/MyDrive/data.csv'"
      ],
      "metadata": {
        "id": "lmeeAgCA7Rl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Читаем CSV файл"
      ],
      "metadata": {
        "id": "1pZLZ6JKfe5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_schema = [\n",
        "               StructField('_c0', IntegerType(), nullable=True),\n",
        "               StructField('symbol', StringType(), True),\n",
        "               StructField('date', DateType(), True),\n",
        "               StructField('open', DoubleType(), True),\n",
        "               StructField('high', DoubleType(), True),\n",
        "               StructField('low', DoubleType(), True),\n",
        "               StructField('close', DoubleType(), True),\n",
        "               StructField('volume', IntegerType(), True),\n",
        "               StructField('adjusted', DoubleType(), True),\n",
        "               StructField('market.cap', StringType(), True),\n",
        "               StructField('sector', StringType(), True),\n",
        "               StructField('industry', StringType(), True),\n",
        "               StructField('exchange', StringType(), True),\n",
        "            ]\n",
        "\n",
        "final_struc = StructType(fields = data_schema)\n",
        "\n",
        "data = spark.read.csv(\n",
        "    df_path,\n",
        "    sep=',',\n",
        "    header=True,\n",
        "    schema=final_struc\n",
        ")"
      ],
      "metadata": {
        "id": "WmJ32VA_TKZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Различные методы инспекции данных\n",
        "\n",
        "data.schema - тот метод возвращает схему данных (фрейма данных). Ниже показан пример с ценами на акции.\n",
        "\n",
        "data.dtypes - возвращает список кортежей с именами столбцов и типами данных.\n",
        "\n",
        "data.show() - по умолчанию отображает первые 20 строк, а также принимает число в качестве параметра для выбора их количества.\n",
        "\n",
        "data.head(n) - возвращает n строк в виде списка.\n",
        "\n",
        "data.first() - возвращает первую строку данных.\n",
        "\n",
        "data.take(n) - возвращает первые n строк.\n",
        "\n",
        "data.describe() - вычисляет некоторые статистические значения для столбцов с числовым типом данных.\n",
        "\n",
        "data.columns - возвращает список, содержащий названия столбцов.\n",
        "\n",
        "data.count() - возвращает общее число строк в датасете.\n",
        "\n",
        "data.distinct() — количество различных строк в используемом наборе данных.\n",
        "\n",
        "data.printSchema() - отображает схему данных."
      ],
      "metadata": {
        "id": "jIeZEeI9hnHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "FDwtlrE5hUtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Манипуляции со столбцами\n",
        "\n",
        "5.1. Добавление столбца: используйте withColumn, чтобы добавить новый столбец к существующим. Метод принимает два параметра: имя столбца и данные.\n",
        "\n",
        "`data = data.withColumn('new_date_column', data.date)`\n",
        "\n",
        "5.2. Обновление столбца: используйте withColumnRenamed, чтобы переименовать существующий столбец. Метод принимает два параметра: название существующего столбца и его новое имя.\n",
        "\n",
        "`data = data.withColumnRenamed('date', 'date_changed')`\n",
        "\n",
        "5.3. Удаление столбца: используйте метод drop, который принимает имя столбца и возвращает данные.\n",
        "\n",
        "`data = data.drop('date_changed')`"
      ],
      "metadata": {
        "id": "iU4CqUNejkeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "1. Прочитать уже знакомый нам датасет stocks_price_final;\n",
        "\n",
        "2. Создать в конце новый столбец new_volume с данными из столбца volume;\n",
        "\n",
        "3. Избавиться от следующих столбцов: symbol, close, volume , adjusted, market.cap, exchange;\n",
        "\n",
        "4. Поменять местами названия столбцов high и low;\n",
        "\n",
        "5. Столбец date переместить в конец;\n",
        "\n",
        "6. Вывести первые 3 строки, скопировать вывод и вставить в ответ (в ответ включить \"only showing top 3 rows\");"
      ],
      "metadata": {
        "id": "gH7a72CZsqxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumn('new_volume', data.volume)\n",
        "data = data.drop('symbol', 'close', 'volume', 'adjusted', 'market.cap', 'exchange')"
      ],
      "metadata": {
        "id": "XtJzQjsvkitY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumnRenamed('high', 'low_new')\n",
        "data = data.withColumnRenamed('low', 'high_new')\n",
        "data = data.withColumnRenamed('low_new', 'low')\n",
        "data = data.withColumnRenamed('high_new', 'high')"
      ],
      "metadata": {
        "id": "pPstBVJ3luM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumn('new_date', data.date)\n",
        "data = data.drop('date')\n",
        "data = data.withColumnRenamed('new_date', 'date')\n",
        "data.show(3)"
      ],
      "metadata": {
        "id": "LN52KoqymjNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Работа с недостающими значениями\n",
        "\n",
        "Мы часто сталкиваемся с отсутствующими значениями при работе с данными реального времени. Эти пропущенные значения обозначаются как NaN, пробелы или другие заполнители. Существуют различные методы работы с пропущенными значениями, некоторые из самых популярных:\n",
        "\n",
        "1. Удаление: удалить строки с пропущенными значениями в любом из столбцов.\n",
        "\n",
        "2. Замена средним/медианным значением: замените отсутствующие значения, используя среднее или медиану соответствующего столбца. Это просто, быстро и хорошо работает с небольшими наборами числовых данных.\n",
        "\n",
        "3. Замена на наиболее частые значения: как следует из названия, используйте наиболее часто встречающееся значение в столбце, чтобы заменить отсутствующие. Это хорошо работает с категориальными признаками, но также может вносить смещение (bias) в данные.\n",
        "\n",
        "4. Замена с использованием KNN: метод K-ближайших соседей — это алгоритм классификации, который рассчитывает сходство признаков новых точек данных с уже существующими, используя различные метрики расстояния, такие как Евклидова, Махаланобиса, Манхэттена, Минковского, Хэмминга и другие. Такой подход более точен по сравнению с вышеупомянутыми методами, но он требует больших вычислительных ресурсов и довольно чувствителен к выбросам.\n",
        "\n",
        "Давайте посмотрим, как мы можем использовать PySpark для решения проблемы отсутствующих значений:\n",
        "\n",
        "\n",
        "`import pyspark.sql.functions as f`\n",
        "\n",
        "6.1. Удаление строк с пропущенными значениями\n",
        "\n",
        "`data.na.drop()`\n",
        "\n",
        "6.2. Замена отсутствующих значений средним\n",
        "\n",
        "`data.na.fill(data.select(f.mean(data['open'])).collect()[0][0])`\n",
        "\n",
        "6.3. Замена отсутствующих значений новыми\n",
        "\n",
        "`data.na.replace(old_value, new_value)`"
      ],
      "metadata": {
        "id": "GuNE3eG7tXJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "1. Прочитать уже знакомый нам датасет stocks_price_final;\n",
        "\n",
        "2. Вычислить разницу между исходным количеством строк в датасете и количеством строк после удаления пропущенных значений."
      ],
      "metadata": {
        "id": "ttTbnRRruDfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_schema = [\n",
        "               StructField('_c0', IntegerType(), nullable=True),\n",
        "               StructField('symbol', StringType(), True),\n",
        "               StructField('date', DateType(), True),\n",
        "               StructField('open', DoubleType(), True),\n",
        "               StructField('high', DoubleType(), True),\n",
        "               StructField('low', DoubleType(), True),\n",
        "               StructField('close', DoubleType(), True),\n",
        "               StructField('volume', IntegerType(), True),\n",
        "               StructField('adjusted', DoubleType(), True),\n",
        "               StructField('market.cap', StringType(), True),\n",
        "               StructField('sector', StringType(), True),\n",
        "               StructField('industry', StringType(), True),\n",
        "               StructField('exchange', StringType(), True),\n",
        "            ]\n",
        "\n",
        "final_struc = StructType(fields = data_schema)\n",
        "\n",
        "data = spark.read.csv(df_path, sep=',', header=True, schema=final_struc).withColumnRenamed('market.cap','market_cap')"
      ],
      "metadata": {
        "id": "UAYFhg94uLni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pr = data.count()\n",
        "data_dr = data.na.drop().count()\n",
        "data_pr - data_dr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME02Y4HUvHQi",
        "outputId": "21dc5dde-26dc-42cf-cf2c-782f04261fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3827"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Получение данных\n",
        "\n",
        "7.1. **Select** - используется для выбора одного или нескольких столбцов, используя их имена.\n",
        "\n",
        "7.1.1. Выбор одного столбца\n",
        "\n",
        "`data.select('sector').show(5)`\n",
        "\n",
        "7.1.2. Выбор нескольких столбцов\n",
        "\n",
        "`data.select(['open', 'close', 'adjusted']).show(5)`\n",
        "\n",
        "\n",
        "7.2. **Filter** - Данный метод фильтрует данные на основе заданного условия. Вы также можете указать несколько условий, используя операторы AND (&), OR (|) и NOT (~). Вот пример получения данных о ценах на акции за январь 2020 года.\n",
        "\n",
        "`data.filter( (col('data') >= lit('2020-01-01')) & (col('data') <= lit('2020-01-31')) ).show(5)`\n",
        "\n",
        "\n",
        "7.3. **Between** - Этот метод возвращает True, если проверяемое значение принадлежит указанному отрезку, иначе — False. Давайте посмотрим на пример отбора данных, в которых значения adjusted находятся в диапазоне от 100 до 500.\n",
        "\n",
        "`data.filter(data.adjusted.between(100.0, 500.0)).show()`\n",
        "\n",
        "\n",
        "7.4. **When** - Он возвращает 0 или 1 в зависимости от заданного условия. В приведенном ниже примере показано, как выбрать такие цены на момент открытия и закрытия торгов, при которых скорректированная цена была больше или равна 200.\n",
        "```\n",
        "data.select('open', 'close', when(data.adjusted >= 200.0, 1).otherwise(0)).show(5)\n",
        "```\n",
        "\n",
        "\n",
        "7.5. **Like** - Этот метод похож на оператор Like в SQL. Приведенный ниже код демонстрирует использование rlike() для извлечения имен секторов, которые начинаются с букв M или C.\n",
        "\n",
        "```\n",
        "data.select('sector', data.sector.rlike('^[B,C]').alias('Колонка sector начинается с B или C')).distinct().show()\n",
        "```\n",
        "\n",
        "\n",
        "7.6. **GroupBy** - Само название подсказывает, что данная функция группирует данные по выбранному столбцу и выполняет различные операции, такие как вычисление суммы, среднего, минимального, максимального значения и т. д. В приведенном ниже примере объясняется, как получить среднюю цену открытия, закрытия и скорректированную цену акций по отраслям.\n",
        "\n",
        "```\n",
        "(data.select(['industry', 'open', 'close', 'adjusted'])\n",
        "    .groupBy('industry')\n",
        "    .mean()\n",
        "    .show()\n",
        ")\n",
        "```\n",
        "\n",
        "7.7. **Агрегирование** - PySpark предоставляет встроенные стандартные функции агрегации, определенные в API DataFrame, они могут пригодится, когда нам нужно выполнить агрегирование значений ваших столбцов. Другими словами, такие функции работают с группами строк и вычисляют единственное возвращаемое значение для каждой группы.\n",
        "\n",
        "В приведенном ниже примере показано, как отобразить минимальные, максимальные и средние значения цен открытия, закрытия и скорректированных цен акций в промежутке с января 2019 года по январь 2020 года для каждого сектора.\n",
        "\n",
        "```\n",
        "(data.filter((col(\"data\") >= lit(\"2019-01-02\")) & (col(\"data\") <= lit(\"2020-01-31\")))\n",
        "    .groupBy(\"sector\")\n",
        "    .agg(min(\"data\").alias(\"С\"), \n",
        "         max(\"data\").alias(\"По\"), \n",
        "         \n",
        "         min(\"open\").alias(\"Минимум при открытии\"),\n",
        "         max(\"open\").alias(\"Максимум при открытии\"), \n",
        "         avg(\"open\").alias(\"Среднее в open\"), \n",
        "\n",
        "         min(\"close\").alias(\"Минимум при закрытии\"), \n",
        "         max(\"close\").alias(\"Максимум при закрытии\"), \n",
        "         avg(\"close\").alias(\"Среднее в close\"), \n",
        "\n",
        "         min(\"adjusted\").alias(\"Скорректированный минимум\"), \n",
        "         max(\"adjusted\").alias(\"Скорректированный максимум\"), \n",
        "         avg(\"adjusted\").alias(\"Среднее в adjusted\"), \n",
        "\n",
        "      ).show(truncate=False)\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OJlgPg1gyDsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Сгруппировать значения по возрасту и найти количество элементов в группе."
      ],
      "metadata": {
        "id": "slhhY6XneGI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_bank = spark.read.csv(df_path2, sep=';', header=True)"
      ],
      "metadata": {
        "id": "TO_Y9vi6eJyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data_bank\n",
        "  .groupBy('age')\n",
        "  .agg(count('age').alias('count'))\n",
        "  .show(5)\n",
        ")"
      ],
      "metadata": {
        "id": "YKpjwfPBmIme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Найти, сотрудники с каким возрастом работают в банке чаще всего."
      ],
      "metadata": {
        "id": "aFVJM-X3mHkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(data_bank\n",
        "  .groupBy('age')\n",
        "  .agg(count('age').alias('count'))\n",
        "  .sort(desc('count'))\n",
        "  .show(5)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jkuWUT-ebVt",
        "outputId": "cbab4749-620e-482d-81ff-172437d13133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 34|  231|\n",
            "| 32|  224|\n",
            "| 31|  199|\n",
            "| 36|  188|\n",
            "| 33|  186|\n",
            "+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Необходимо найти возраст и количество самых молодых сотрудников банка"
      ],
      "metadata": {
        "id": "Qkuz6ne4nY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(data_bank\n",
        "  .groupBy('age')\n",
        "  .agg(count('age').alias('count'))\n",
        "  .sort(asc('age'))\n",
        "  .show(5)\n",
        ")"
      ],
      "metadata": {
        "id": "fiMnpyventpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Необходимо вывести возраст и количество сотрудников, которым более 30 лет. Произвести сортировку полученной таблицы по столбцу age по возрастанию."
      ],
      "metadata": {
        "id": "BSMjz-GAnsQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "data_bank = data_bank.withColumn(\"age\", data_bank[\"age\"].cast(IntegerType()))\n",
        "\n",
        "(data_bank.filter(col('age') > 30)\n",
        "  .groupBy('age')\n",
        "  .agg(count('age').alias('count'))\n",
        "  .sort(asc('age'))\n",
        "  .show(5)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F16hWhFnaI-",
        "outputId": "64586427-36ec-4a66-8aae-d4bfd800e6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 31|  199|\n",
            "| 32|  224|\n",
            "| 33|  186|\n",
            "| 34|  231|\n",
            "| 35|  180|\n",
            "+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Визуализация данных\n",
        "\n",
        "Для визуализации данных мы воспользуемся библиотеками matplotlib и pandas. Метод toPandas() позволяет нам осуществить преобразование данных в dataframe pandas, который мы используем при вызове метода визуализации plot(). В приведенном ниже коде показано, как отобразить гистограмму, отображающую средние значения цен открытия, закрытия и скорректированных цен акций для каждого сектора.\n",
        "\n",
        "```\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "sec_df =  data.select(['sector', \n",
        "                       'open', \n",
        "                       'close', \n",
        "                       'adjusted']\n",
        "                     )\\\n",
        "                     .groupBy('sector')\\\n",
        "                     .mean()\\\n",
        "                     .toPandas()\n",
        "\n",
        "ind = list(range(12))\n",
        "ind.pop(6)\n",
        "\n",
        "sec_df.iloc[ind,:].plot(kind='bar', x='sector', y=sec_df.columns.tolist()[1:], \n",
        "                         figsize=(12, 6), ylabel='Stock Price', xlabel='Sector')\n",
        "plt.show()\n",
        "```\n",
        "Теперь давайте визуализируем те же средние показатели, но уже по отраслям.\n",
        "\n",
        "\n",
        "```\n",
        "industries_x = data.select(['industry', 'open', 'close', 'adjusted']).groupBy('industry').mean().toPandas()\n",
        "\n",
        "q  = industries_x[(industries_x.industry != 'Major Chemicals') & (industries_x.industry != 'Building Products')]\n",
        "q.plot(kind='barh', x='industry', y=q.columns.tolist()[1:], figsize=(10, 50), xlabel='Stock Price', ylabel='Industry')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Также построим временные ряды для средних цен открытия, закрытия и скорректированных цен акций технологического сектора."
      ],
      "metadata": {
        "id": "cb0UfCTbqUo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Отобразить изменение средних значений аудио характеристик от года к году.\n",
        "\n",
        "Такими характеристиками являются acousticness, danceability, energy, speechiness, liveness и valence . Произвести сортировку полученной таблицы по столбцу year по возрастанию. Средние значения округлить до 2ух знаков после запятой."
      ],
      "metadata": {
        "id": "fVuyrSayroEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_e = (\n",
        "    spark.read.option(\"delimiter\", \",\")\n",
        "    .option(\"header\", True)\n",
        "    .option(\"escape\", '\"')\n",
        "    .csv(df_path3)\n",
        ")"
      ],
      "metadata": {
        "id": "7_hqWoLurpXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data_e\n",
        "  .groupBy('year')\n",
        "  .agg(\n",
        "      round(avg('acousticness'),2).alias('acousticness'),\n",
        "      round(avg('danceability'),2).alias('danceability'),\n",
        "      round(avg('energy'),2).alias('energy'),\n",
        "      round(avg('liveness'),2).alias('liveness'),\n",
        "      round(avg('speechiness'),2).alias('speechiness'),\n",
        "      round(avg('valence'),2).alias('valence')\n",
        "  )\n",
        "  .sort(asc('year'))\n",
        "  .show(5)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-1152pP4h_b",
        "outputId": "7499dd1a-fe5f-431b-8fe5-b9aa4e7458fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+------------+------+--------+-----------+-------+\n",
            "|year|acousticness|danceability|energy|liveness|speechiness|valence|\n",
            "+----+------------+------------+------+--------+-----------+-------+\n",
            "|1921|         0.9|        0.43|  0.24|    0.22|       0.08|   0.43|\n",
            "|1922|        0.94|        0.48|  0.24|    0.24|       0.12|   0.53|\n",
            "|1923|        0.98|        0.57|  0.25|    0.24|        0.1|   0.62|\n",
            "|1924|        0.94|        0.55|  0.35|    0.24|       0.09|   0.67|\n",
            "|1925|        0.97|        0.57|  0.26|    0.24|       0.12|   0.62|\n",
            "+----+------------+------------+------+--------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Найти количество произведений, выпущенных с 1951 года, в авторах которых присутствует \"Sergei Rachmaninoff\"."
      ],
      "metadata": {
        "id": "Mc4kC_tuAqo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"id\", StringType(), nullable=True),\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"artists\", StringType(), True),\n",
        "        StructField(\"duration_ms\", DoubleType(), True),\n",
        "        StructField(\"release_date\", DateType(), True),\n",
        "        StructField(\"year\", IntegerType(), True),\n",
        "        StructField(\"acousticness\", StringType(), True),\n",
        "        StructField(\"danceability\", StringType(), True),\n",
        "        StructField(\"energy\", StringType(), True),\n",
        "        StructField(\"instrumentalness\", StringType(), True),\n",
        "        StructField(\"liveness\", StringType(), True),\n",
        "        StructField(\"loudness\", StringType(), True),\n",
        "        StructField(\"speechiness\", StringType(), True),\n",
        "        StructField(\"tempo\", StringType(), True),\n",
        "        StructField(\"valence\", StringType(), True),\n",
        "        StructField(\"mode\", StringType(), True),\n",
        "        StructField(\"key\", StringType(), True),\n",
        "        StructField(\"popularity\", DoubleType(), True),\n",
        "        StructField(\"explicit\", StringType(), True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_e2 = (\n",
        "    spark.read.option(\"delimiter\", \",\")\n",
        "    .option(\"header\", True)\n",
        "    .option(\"escape\", '\"')\n",
        "    .schema(schema)\n",
        "    .csv(df_path3)\n",
        ")"
      ],
      "metadata": {
        "id": "pO0YqwqfAsLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_sr = data_e2.filter( (data_e2.artists.contains('Sergei Rachmaninoff')) & (data_e2.year > 1951))\n",
        "count_sr.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW1HPn5SBIx6",
        "outputId": "554c1849-48e1-463a-830b-6ac546996d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача**\n",
        "\n",
        "Найти наиболее популярных артистов (средняя популярность всех произведений, в которых упомянут артист).\n",
        "\n",
        "Выборку производить из тех артистов, у которых общее количество упоминаний в произведениях не менее 200."
      ],
      "metadata": {
        "id": "0ElQD8_rCvU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"id\", StringType(), nullable=True),\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"artists\", StringType(), True),\n",
        "        StructField(\"duration_ms\", DoubleType(), True),\n",
        "        StructField(\"release_date\", DateType(), True),\n",
        "        StructField(\"year\", IntegerType(), True),\n",
        "        StructField(\"acousticness\", StringType(), True),\n",
        "        StructField(\"danceability\", StringType(), True),\n",
        "        StructField(\"energy\", StringType(), True),\n",
        "        StructField(\"instrumentalness\", StringType(), True),\n",
        "        StructField(\"liveness\", StringType(), True),\n",
        "        StructField(\"loudness\", StringType(), True),\n",
        "        StructField(\"speechiness\", StringType(), True),\n",
        "        StructField(\"tempo\", StringType(), True),\n",
        "        StructField(\"valence\", StringType(), True),\n",
        "        StructField(\"mode\", StringType(), True),\n",
        "        StructField(\"key\", StringType(), True),\n",
        "        StructField(\"popularity\", DoubleType(), True),\n",
        "        StructField(\"explicit\", StringType(), True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_fn = (\n",
        "    spark.read.option(\"delimiter\", \",\")\n",
        "    .option(\"header\", True)\n",
        "    .option(\"escape\", '\"')\n",
        "    .schema(schema)\n",
        "    .csv(df_path3)\n",
        ")"
      ],
      "metadata": {
        "id": "QbEXszH-CwbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# мой вариант решения\n",
        "from pyspark.sql.functions import split, explode\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import trim\n",
        "\n",
        "\n",
        "# отбор столбцов 'artists', 'popularity'\n",
        "data_f = data_fn.select(['artists', 'popularity'])\n",
        "\n",
        "\n",
        "# замена лишних символов в колонке 'artists'\n",
        "art_rep = f.regexp_replace(f.col(\"artists\"), \"[\\[\\]\\'\\\"]\", \"\")\n",
        "data_f = data_f.withColumn('artists_one', art_rep)\n",
        "#data_f.show(5)\n",
        "\n",
        "\n",
        "# разбите записей по одному исполнителю\n",
        "data_f = data_f.select(\n",
        "    explode(split(col(\"artists_one\"), \",\"))\n",
        "    .alias(\"artists\"),\n",
        "    'popularity'\n",
        "    ).sort(desc('popularity'))\n",
        "#data_f.show(5)\n",
        "\n",
        "\n",
        "# убираем лишние пробелы по краям\n",
        "data_f = data_f.withColumn(\"artist\", trim(data_f.artists))\n",
        "#data_f.show(5)\n",
        "\n",
        "\n",
        "data_f = (\n",
        "    data_f.groupBy('artist')\n",
        "    .agg(\n",
        "        avg('popularity').alias('popularity'),\n",
        "        count('popularity').alias('count')\n",
        "    )\n",
        "    .sort(desc('popularity'))\n",
        "    .filter(col('count') > 200)\n",
        ")\n",
        "\n",
        "data_f.show(5)\n",
        "data_f.select('artist').show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXNyl3ZYsOQe",
        "outputId": "483570ea-3225-4fff-b76f-4a6d9fedf1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+-----+\n",
            "|      artist|        popularity|count|\n",
            "+------------+------------------+-----+\n",
            "|       Drake| 61.60567823343849|  317|\n",
            "|  Kanye West| 58.10762331838565|  223|\n",
            "|Taylor Swift|57.367149758454104|  207|\n",
            "|      Eminem|56.382838283828384|  303|\n",
            "|   Lil Wayne|  54.1501976284585|  253|\n",
            "+------------+------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+\n",
            "|      artist|\n",
            "+------------+\n",
            "|       Drake|\n",
            "|  Kanye West|\n",
            "|Taylor Swift|\n",
            "|      Eminem|\n",
            "|   Lil Wayne|\n",
            "+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# второй вариант решения от stepik\n",
        "\n",
        "data_f = data_f.withColumn(\n",
        "    \"artists\", f.split(f.regexp_replace(f.col(\"artists\"), \"[\\]\\[']\", \"\"), \", \")\n",
        ")\n",
        "\n",
        "data_f = (\n",
        "    data_f.withColumn(\"artists_exp\", f.explode(\"artists\"))\n",
        "    .withColumn(\n",
        "        \"artists_exp\",\n",
        "        f.explode(f.split(f.regexp_replace(\"artists_exp\", \"[\\\\[\\\\]]\", \"\"), \",\")),\n",
        "    )\n",
        "    .groupBy(\"artists_exp\")\n",
        "    .agg(f.avg(\"popularity\"), f.count(\"*\").alias(\"count\"))\n",
        ")\n",
        "\n",
        "data_f = (\n",
        "    data_f.filter(f.col(\"count\") >= 200)\n",
        "    .orderBy(f.col(\"avg(popularity)\").desc())\n",
        "    .withColumnRenamed(\"artists_exp\", \"artist\")\n",
        ")\n",
        "\n",
        "data_f.select(\"artist\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh9eMJCfteGv",
        "outputId": "73599c1b-e91d-49ff-ef5e-3933bbd5e34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|      artist|\n",
            "+------------+\n",
            "|       Drake|\n",
            "|  Kanye West|\n",
            "|Taylor Swift|\n",
            "|      Eminem|\n",
            "|   Lil Wayne|\n",
            "+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}